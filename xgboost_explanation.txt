XGBoost Optimizer Implementation

Overview:
- File: xgboost_optimizer.py
- Purpose: Use XGBoost (XGBRegressor) to model compilation flag configurations -> execution time and guide search for an improved set of GCC flags for a given C/C++ source file.

Inputs/Outputs:
- Inputs: path to a .c/.cpp source file and optional test input (stdin string).
- Outputs: creates `optimized_xgb` binary (if successful) and writes `xgboost_results.json` containing:
  - best_time
  - total_evaluations
  - enabled_flags

Key components and flow:
1. Search space:
   - `GCC_FLAGS`: a fixed list (~80+) of individual GCC flags.
   - A configuration is a binary vector where 1 = enable flag.

2. Phases:
   - Phase 1: Random sampling
     - Generate INITIAL_RANDOM_SAMPLES (100) random binary configurations.
     - Compile and run each config, store successful configurations and measured execution times.
   - Phase 2: Train XGBoost
     - Build X (configs) and y (execution times) arrays.
     - If >5 samples, split train/test else train on all data.
     - Train XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1).
   - Phase 3: Guided optimization
     - For BOOSTING_ROUNDS (50) iterations:
       - Generate random config, predict execution time with the trained model.
       - If predicted time < current best_time, actually evaluate (compile+run) the config.
       - Update best if improved.

3. Evaluation details:
   - Compilation and execution use subprocess.run with timeouts (COMPILATION_TIMEOUT=30s, EXECUTION_TIMEOUT=10s).
   - Returns float('inf') for failed compilation, execution errors, or timeouts.
   - Best config and time are tracked during evaluation.

4. Output and persistence:
   - `print_results` prints best config and creates `optimized_xgb` binary using the best flags.
   - `save_results` writes `xgboost_results.json`.

Limitations/Notes:
- The model is only used as a filter; the guided search still samples random configurations and does not use the model to propose optimal candidates directly.
- Execution time is measured by a single wall-clock run (no repeats or median), making measurements noisy.
- No feature importance or explicit dimensionality reduction is performed.
- Temporary binaries are cleaned up in finally blocks, but edge cases may leave files behind.

Hyperparameters used:
- INITIAL_RANDOM_SAMPLES = 100
- BOOSTING_ROUNDS = 50
- XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1)
- COMPILATION_TIMEOUT = 30s, EXECUTION_TIMEOUT = 10s

Recommendations:
- Use repeated runs (median) to reduce noise.
- Use the model actively to propose candidates (e.g., sample many random configs and select lowest predicted times, or use optimization guided by the model).
- Save the trained model to disk for debugging and reuse.
- Add guard for no valid samples found to avoid crashes.
