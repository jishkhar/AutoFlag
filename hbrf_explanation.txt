HBRF (Hybrid Bayesian-Random Forest) Optimizer Implementation

Overview:
- File: hbrf_optimizer.py
- Purpose: Hybrid approach: random sampling -> Random Forest feature importance -> Bayesian optimization (gp_minimize) on reduced flag subset -> greedy local refinement. Designed to find improved GCC flag sets for a provided C/C++ source file.

Inputs/Outputs:
- Inputs: path to a .c/.cpp source file and optional test input (stdin string).
- Outputs: creates `optimized_hbrf` binary (if successful) and writes `hbrf_results.json` containing:
  - best_time
  - total_optimization_time
  - total_evaluations
  - enabled_flags
  - flag_importance

Key components and flow:
1. Preconditions:
   - Requires scikit-optimize (`skopt`) and scikit-learn (`sklearn`). The constructor raises ImportError if not installed.

2. Search space and representation:
   - Shared `GCC_FLAGS` list (~80+ flags).
   - Configuration represented as a binary vector of length len(GCC_FLAGS).

3. 4 Phases:
   - Phase 1: Random sampling
     - Collect INITIAL_RANDOM_SAMPLES (100) random configurations, evaluate by compiling and running, store successful ones.
   - Phase 2: Random Forest feature importance
     - Train RandomForestRegressor(n_estimators=100, max_depth=10) on collected samples.
     - Compute `feature_importances_` and select TOP_FLAGS_COUNT (20) most important flags; store importance scores in `flag_importance`.
   - Phase 3: Bayesian Optimization on reduced space
     - Define a discrete search space with one Integer(0,1) per top-flag index.
     - Use `gp_minimize` with `n_calls=BAYESIAN_ITERATIONS` (50) to minimize execution time.
     - Objective builds a full-length config where only top flags vary; each gp call evaluates the configuration (compile+run).
   - Phase 4: Greedy refinement
     - For up to FINAL_GREEDY_ADDITIONS (10) randomly chosen non-top flags, attempt adding/removing the flag to/from the best config and accept strictly improving moves.

4. Evaluation and persistence:
   - Compilation and execution use subprocess.run with timeouts (COMPILATION_TIMEOUT=30s, EXECUTION_TIMEOUT=10s).
   - `evaluate_configuration` updates `best_time` and `best_config` on successful improvements.
   - After the run, `print_results` compiles `optimized_hbrf`, prints details including importances, and writes `hbrf_results.json`.

Important implementation notes / limitations:
- Bayesian optimization is run over a reduced, binary space (TOP_FLAGS_COUNT dims). GPs on binary inputs can be less effective than for continuous spaces; still, reduction improves practicality.
- If insufficient random samples are collected (fewer than ~10), code returns all flags (no reduction), which may make BO expensive.
- Execution time labeling uses a single run per config (no repeats), so measurements are noisy.
- If no valid configuration is found in Phase 1, `best_config` stays None and later phases that do `self.best_config.copy()` could fail.
- gp_minimize is executed with n_jobs=1 and random_state=42.

Hyperparameters used:
- INITIAL_RANDOM_SAMPLES = 100
- TOP_FLAGS_COUNT = 20
- BAYESIAN_ITERATIONS = 50
- FINAL_GREEDY_ADDITIONS = 10
- RandomForestRegressor(n_estimators=100, max_depth=10)

Recommendations:
- Reduce noise by repeating runs and using median or trimmed mean for target values.
- Add a guard if no valid samples are found (fail early with a helpful message).
- Consider alternate BO surrogate models or surrogate encodings for binary variables (e.g., tree-based BO or TPE/SMAC style search) which naturally handle categorical/binary domains.
- Save models (RF and BO results) to disk for later analysis.
- Add unit/integration tests using a tiny, fast-to-run C program to validate end-to-end pipeline in CI.
